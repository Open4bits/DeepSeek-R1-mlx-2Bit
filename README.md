## Model Hosting (Warning: Absolute Unit)

This repository does **NOT** contain the model weight files.

Why?

Because the model is a **210GB monster** and GitHub would probably faint if we tried.

The quantized 2-bit version of **DeepSeek-R1** is hosted safely on Hugging Face (where strong servers live):

ðŸ‘‰ [https://huggingface.co/Open4bits/DeepSeek-R1-mlx-2Bit](https://huggingface.co/Open4bits/DeepSeek-R1-mlx-2Bit)

If your internet starts crying while downloadingâ€¦ thatâ€™s normal.
Stay hydrated. Youâ€™ll be there for a while.
